{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493a660d-6127-484e-a5cc-557cc197e8f4",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 1: Intro to TensorFlow and Music Generation with RNNs\n",
    "## Part 2: Music Generation with RNNs\n",
    "\n",
    "In this project, get ready to embark on an exciting journey to explore the realm of music generation using Recurrent Neural Networks (RNNs). Inspired by the MIT Introduction to Deep Learning course, I set out to build a system that could compose music in ABC notation, a symbolic music notation system. This notation allows us to represent melodies as text, making it a convenient format for both human interpretation and machine learning applications.\n",
    "\n",
    "The original course materials were provided in TensorFlow, a popular deep learning framework. However, I decided to take the initiative to convert the entire project to PyTorch, another widely-used deep learning library known for its dynamic computational graph and ease of use. This conversion not only will broaden our understanding of different machine learning frameworks but will also showcase the flexibility and adaptability required in real-world AI projects.\n",
    "\n",
    "Throughout the project, I followed the implementation done by the MIT instructors of a Long Short-Term Memory (LSTM) network, a type of RNN particularly well-suited for sequence prediction tasks. The LSTM model was trained on a dataset of ABC notated songs, learning the patterns and structures of musical compositions. I was able to convert the code to PyTorch and fine-tuned various hyperparameters, including the embedding dimensions, number of LSTM units, and learning rates, to optimize the model's performance.\n",
    "\n",
    "One of the project's key features was the ability to generate new musical pieces by feeding a seed sequence into the trained model. The model then predicted subsequent notes, effectively composing new music in real-time. We addressed challenges such as handling multiple time signatures and ensuring the generated music maintained a coherent structure.\n",
    "\n",
    "The final model not only demonstrates the power of deep learning in creative applications but also provides a hands-on example of end-to-end machine learning pipeline development, from data processing and model training to deployment and inference.\n",
    "\n",
    "By completing this project, we've gained valuable experience in deep learning, RNNs, music generation, and the practicalities of converting between TensorFlow and PyTorch. This project serves as a testament to the exciting possibilities of AI in the creative arts and the importance of interdisciplinary learning in the rapidly evolving field of artificial intelligence.\n",
    "\n",
    "We will be using Comet ML to track our model development and training runs. First, sign up for a Comet account at this link https://www.comet.com/signup?utm_source=mit_dl&utm_medium=partner&utm_content=github (you can use your Google or Github account). This will generate a personal API Key, which you can find either in the first 'Get Started with Comet' page, under your account settings, or by pressing the '?' in the top right corner and then 'Quickstart Guide'. Enter this API key as the global variable COMET_API_KEY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd60ff6-5408-4a00-8ddc-634fb8680c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install comet_ml > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019caf03-4d6d-4c68-8524-eca10b293405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "# TODO: ENTER YOUR API KEY HERE!! instructions above\n",
    "COMET_API_KEY = \"Your API KEY HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace8259-b404-4ff8-839e-6d104242b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "experiment = Experiment(\n",
    "  api_key=\"YOUR API KEY\",\n",
    "  project_name=\"general\",\n",
    "  workspace=\"algo345\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c8b10-e082-4213-b5cc-980d0533d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and other necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import other packages\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "!apt-get install abcmidi timidity > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40062d-f8ec-4b95-89d8-e37ff5433ffc",
   "metadata": {},
   "source": [
    "Comet ML Integration:\n",
    "\n",
    "    Replace comet_ml.Experiment with your specific project and workspace names.\n",
    "\n",
    "PyTorch vs TensorFlow:\n",
    "\n",
    "    PyTorch uses torch for its operations, including tensor computations, neural network models, etc.\n",
    "\n",
    "Checking GPU Availability:\n",
    "\n",
    "    PyTorch uses torch.cuda.is_available() to check if a GPU is available, and then torch.device(\"cuda\") to set the device.\n",
    "\n",
    "Additional Installations:\n",
    "\n",
    "    The command !apt-get install abcmidi timidity is for installing additional tools and remains the same as it's system-level installation and not dependent on the deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcc939-ad69-4583-9d53-d6d419814bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # Should return True if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec88d9e-668d-4261-8c53-d5722e8efdb8",
   "metadata": {},
   "source": [
    "We've gathered a dataset of thousands of Irish folk songs, represented in the ABC notation. Let's download the dataset and inspect it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071e940-eef4-42d6-921f-7b195494b7fe",
   "metadata": {},
   "source": [
    "This code will download the dataset using the load_training_data function from mitdeeplearning.lab1 and print out an example song from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1768b6d-47f0-4be9-9351-492699ae549d",
   "metadata": {},
   "source": [
    "To convert the utility functions in the mitdeeplearning package to use PyTorch instead of TensorFlow, we'll need to make some modifications to the code. The provided lab1.py file has several functions that depend on TensorFlow, and we'll need to adapt these for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af96451-d00e-45d1-a5aa-52687f04341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cesco345/musicgen/')  # Adjust the path to your directory structure\n",
    "!pip install torchviz\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "# Now you can use mdl as needed, e.g.:\n",
    "songs = mdl.lab1.load_training_data()\n",
    "example_song = songs[0]\n",
    "print(\"\\nExample song: \")\n",
    "print(example_song)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ec98c-2317-4da6-bb7b-d0ff58952e60",
   "metadata": {},
   "source": [
    "We also need to convert the utility functions in util.py from TensorFlow to PyTorch, we need to modify the functions that use TensorFlow, such as display_model and any other function relying on TensorFlow-specific functionality. I have included the PyTorch revised util.py in the repository files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66edac-8a31-4ff6-b5ca-28648747b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808d089-e9b4-4315-b0ac-0d02f830391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ABC notation to audio file and listen to it\n",
    "mdl.lab1.play_song(example_song)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba6dac-a62a-43e0-be30-4b6723cecf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, midi\n",
    "\n",
    "def play_song_abc(abc_str):\n",
    "    score = converter.parse(abc_str, format='abc')\n",
    "    mf = midi.translate.music21ObjectToMidiFile(score)\n",
    "    mf.open('output.midi', 'wb')\n",
    "    mf.write()\n",
    "    mf.close()\n",
    "\n",
    "    # Play the MIDI file using an appropriate method (depends on your system)\n",
    "    os.system('timidity output.midi')\n",
    "\n",
    "example_song = \"\"\"\n",
    "X:1\n",
    "T:Alexander's\n",
    "Z: id:dc-hornpipe-1\n",
    "M:C|\n",
    "L:1/8\n",
    "K:D Major\n",
    "(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\n",
    "dAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\n",
    "AG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\n",
    "FAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\n",
    "\"\"\"\n",
    "play_song_abc(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5754-b712-48a2-a5e9-1cab55b6a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub\n",
    "!pip install midi2audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5c8bd-001e-4c36-a7b8-80152caa4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, midi\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def play_song_abc(abc_str):\n",
    "    # Convert ABC to MIDI\n",
    "    score = converter.parse(abc_str, format='abc')\n",
    "    mf = midi.translate.music21ObjectToMidiFile(score)\n",
    "    midi_filename = 'output.midi'\n",
    "    mf.open(midi_filename, 'wb')\n",
    "    mf.write()\n",
    "    mf.close()\n",
    "\n",
    "    # Convert MIDI to WAV using FluidSynth\n",
    "    sf2_path = '/home/cesco345/musicgen/FluidR3_GM/FluidR3_GM.sf2'  # Replace with your SoundFont file path\n",
    "    if not os.path.isfile(sf2_path):\n",
    "        print(\"SoundFont file not found. Please make sure it is a valid .sf2 file.\")\n",
    "        return\n",
    "    \n",
    "    fs = FluidSynth(sf2_path)\n",
    "    fs.midi_to_audio(midi_filename, 'output.wav')\n",
    "\n",
    "    # Play the WAV file using a suitable method\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(\"output.wav\")\n",
    "        try:\n",
    "            play(audio)  # Using pydub\n",
    "        except:\n",
    "            # Alternative: Using system calls to available players\n",
    "            try:\n",
    "                subprocess.call(['mpg123', 'output.wav'])\n",
    "            except:\n",
    "                try:\n",
    "                    subprocess.call(['play', 'output.wav'])\n",
    "                except:\n",
    "                    print(\"No suitable audio player found. Please manually play the file at 'output.wav'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Audio file not found or error occurred during playback.\")\n",
    "\n",
    "example_song = \"\"\"\n",
    "X:1\n",
    "T:Alexander's\n",
    "Z: id:dc-hornpipe-1\n",
    "M:C|\n",
    "L:1/8\n",
    "K:D Major\n",
    "(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\n",
    "dAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\n",
    "AG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\n",
    "FAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\n",
    "\"\"\"\n",
    "\n",
    "play_song_abc(example_song)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d060d-54fa-4409-b67a-cbd7cde62b0e",
   "metadata": {},
   "source": [
    "To continue with the task, you can use the following code snippet. This code assumes you have a list of song strings, where each song is represented as a string. The code joins all the songs into a single string and then finds all unique characters in that combined string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59596c-e119-4e72-a731-ff6f2e9b7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `songs` is a list of strings, each representing a song\n",
    "# Join our list of song strings into a single string containing all songs\n",
    "songs_joined = \"\\n\\n\".join(songs)\n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(songs_joined))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c0183-f6d8-41b2-bb7d-acb9c1f72b11",
   "metadata": {},
   "source": [
    "To vectorize the text for training the RNN model, you need to create two lookup tables: char2idx for mapping characters to numerical indices and idx2char for mapping indices back to characters. This is crucial for converting the text data into a numerical format that the model can understand and work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc73e7b-d43e-43de-bf3f-0ee53b7fe978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a mapping from character to unique index\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from indices to characters\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Example usage:\n",
    "# To get the index of the character 'd', use char2idx['d']\n",
    "char_index = char2idx['d']\n",
    "\n",
    "# To get the character from the index, use idx2char\n",
    "character = idx2char[char_index]\n",
    "\n",
    "print(f\"Index for 'd': {char_index}\")\n",
    "print(f\"Character for index {char_index}: {character}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565c0f0-765a-461b-aae6-b5f319d63c05",
   "metadata": {},
   "source": [
    "This process converts the text data into a format suitable for feeding into a neural network. Each character is represented by a unique index, allowing the RNN to learn patterns in the sequence of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30290821-4bfa-4649-b1a6-97ff3507bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b7c03-3bd3-4dfe-bc16-f5cc8bd1f79e",
   "metadata": {},
   "source": [
    "The function vectorize_string takes a string and converts it into a numerical representation using the char2idx mapping. This function will map each character in the string to its corresponding index from the char2idx dictionary, creating a NumPy array of these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f40c8-f5bd-4cac-81e7-d5ba83a2b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_string(string):\n",
    "    # Convert each character in the string to its corresponding index\n",
    "    vectorized_output = np.array([char2idx[char] for char in string])\n",
    "    return vectorized_output\n",
    "\n",
    "# Convert the entire songs dataset to its vectorized form\n",
    "vectorized_songs = vectorize_string(songs_joined)\n",
    "\n",
    "print(vectorized_songs[:100])  # Print the first 100 indices to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4262945-b29e-4fdf-9853-ed117e1222d0",
   "metadata": {},
   "source": [
    "The provided code defines a function vectorize_string that converts a string into a numerical representation using a predefined character-to-index mapping (char2idx). This is useful for preparing text data for input into machine learning models, particularly in the context of training a model to generate music or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3fdc7-f746-4ea3-bbf2-a7aa692c4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_string(string):\n",
    "    # Convert each character in the string to its corresponding index\n",
    "    vectorized_output = np.array([char2idx[char] for char in string])\n",
    "    return vectorized_output\n",
    "\n",
    "# Convert the entire songs dataset to its vectorized form\n",
    "vectorized_songs = vectorize_string(songs_joined)\n",
    "\n",
    "# Print the first 100 indices to verify the conversion\n",
    "print(vectorized_songs[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e7c51-e01b-4177-840f-0697e438bce7",
   "metadata": {},
   "source": [
    "To create training examples and targets from the vectorized song data, we need to define a function that generates batches of input and output sequences. This function, get_batch, will select random starting points from the vectorized data and extract sequences of a given length (seq_length). The targets for each input sequence will be the next characters in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3426d6-ed40-42c1-9321-009108b51249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    # the length of the vectorized songs string minus one for the target\n",
    "    n = vectorized_songs.shape[0] - 1\n",
    "    # randomly choose the starting indices for the examples in the training batch\n",
    "    idx = np.random.choice(n-seq_length, batch_size)\n",
    "\n",
    "    # construct a list of input sequences for the training batch\n",
    "    input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
    "    # construct a list of output sequences for the training batch\n",
    "    output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
    "\n",
    "    # x_batch, y_batch provide the true inputs and targets for network training\n",
    "    x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
    "    return x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631bc9e-a535-4f89-b916-81c3fab7deff",
   "metadata": {},
   "source": [
    "To test the function, you would call it with your vectorized song data, desired sequence length, and batch size. The function will return batches of input and output sequences ready for training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29518222-28b2-4bb4-8a59-389e572a8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length and batch size\n",
    "seq_length = 5\n",
    "batch_size = 1\n",
    "\n",
    "# Get a batch of data\n",
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
    "\n",
    "# Iterate through the batch and display the input and expected output\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9db3ad-63c9-4b11-a49a-385319e5f6d0",
   "metadata": {},
   "source": [
    "Embedding Layer: The nn.Embedding layer in PyTorch functions similarly to tf.keras.layers.Embedding, converting input indices to dense vectors of a specified size.\n",
    "\n",
    "LSTM Layer: The nn.LSTM layer in PyTorch corresponds to the LSTM layer in TensorFlow. It is set up with rnn_units as the number of hidden units.\n",
    "\n",
    "Dense Layer: The nn.Linear layer acts like the dense layer in TensorFlow, converting the LSTM outputs into vocabulary-size logits.\n",
    "\n",
    "Hidden State Initialization: The init_hidden method initializes the hidden state of the LSTM, with the dimensions matching the batch size and number of LSTM units. This is required for the initial state when the model starts processing a new sequence.\n",
    "\n",
    "Forward Pass: In the forward method, the input data passes through the embedding layer, then the LSTM layer, and finally the dense layer to produce the output logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16196551-5411-4c96-be1d-633446491f3e",
   "metadata": {},
   "source": [
    "The MusicRNN class mirrors the architecture defined in the TensorFlow model, providing equivalent functionality in PyTorch. You can now use this model for training and prediction tasks in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76eb53-45cf-4d67-b5d7-f76db2998119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.dense(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, rnn_units).to(device),\n",
    "                torch.zeros(1, batch_size, rnn_units).to(device))\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "vocab_size = len(vocab)  # Size of the vocabulary\n",
    "\n",
    "# Instantiate the model\n",
    "model = MusicRNN(vocab_size, embedding_dim, rnn_units).to(device)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8212b-0718-43d2-8d81-189e5da17cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.dense(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return (torch.zeros(1, self.batch_size, self.rnn_units).to(device),\n",
    "                torch.zeros(1, self.batch_size, self.rnn_units).to(device))\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "vocab_size = len(vocab)  # Size of the vocabulary\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Instantiate the model\n",
    "model = MusicRNN(vocab_size, embedding_dim, rnn_units, batch_size).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a9f38-ebcc-403d-9b28-14f7f3c0010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.dense(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return (torch.zeros(1, self.batch_size, self.rnn_units).to(device),\n",
    "                torch.zeros(1, self.batch_size, self.rnn_units).to(device))\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "vocab_size = 83  # Size of the vocabulary\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Instantiate the model\n",
    "model = MusicRNN(vocab_size, embedding_dim, rnn_units, batch_size).to(device)\n",
    "\n",
    "# Display the model architecture\n",
    "print(model)\n",
    "\n",
    "# Create a dummy input tensor with the correct type and dimensions\n",
    "dummy_input = torch.zeros(batch_size, 100, dtype=torch.long).to(device)\n",
    "\n",
    "# Forward pass to inspect the outputs\n",
    "logits, hidden = model(dummy_input)\n",
    "\n",
    "# Print out the shapes of the outputs\n",
    "print(f'Logits shape: {logits.shape}')\n",
    "print(f'Hidden state shape: {hidden[0].shape}, Cell state shape: {hidden[1].shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592a0e5-7119-4485-b1b2-9f4123c35ad2",
   "metadata": {},
   "source": [
    "The model summary and output shapes indicate that your MusicRNN model is set up correctly:\n",
    "\n",
    "    Embedding Layer: Maps the input indices to a 256-dimensional vector for each character in the vocabulary.\n",
    "    LSTM Layer: Processes the sequence of embedded vectors and outputs a sequence of hidden states.\n",
    "    Dense Layer: Maps the output of the LSTM to the size of the vocabulary, providing logits for each character in the vocabulary at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2864e-1d27-48cf-a2b9-38f526508073",
   "metadata": {},
   "source": [
    "The output shapes are as follows:\n",
    "\n",
    "    Logits: The shape [32, 100, 83] indicates a batch size of 32, a sequence length of 100, and 83 possible character classes.\n",
    "    Hidden State & Cell State: Both have shapes [1, 32, 1024], corresponding to the number of layers (1), batch size (32), and hidden units (1024) in the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba359f5-a1c0-4767-a1c5-9af401a2edfe",
   "metadata": {},
   "source": [
    "To use the get_batch function to obtain a batch of inputs and targets, and then run these inputs through the model, you need to ensure the inputs are moved to the correct device (CPU or GPU) and have the appropriate data type. Here's how you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b8171-a821-4330-a062-4d85b031b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming get_batch function and vectorized_songs are defined as before\n",
    "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
    "\n",
    "# Ensure x is a LongTensor, as required for the Embedding layer\n",
    "x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "\n",
    "# Pass the batch through the model\n",
    "pred, hidden = model(x)\n",
    "\n",
    "# Output the shapes\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2165ae8-3cf7-45a3-b14a-c09d1398e582",
   "metadata": {},
   "source": [
    "To generate predictions from the untrained model, we'll sample from the softmax distribution of the model's output to get actual character indices. Here's how you can implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16801783-594f-47f0-81cc-a6e6f2c558ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming `pred` contains the logits output from the model\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probs = F.softmax(pred[0], dim=-1)\n",
    "\n",
    "# Sample from the distribution to get the predicted character index at each timestep\n",
    "sampled_indices = torch.multinomial(probs, num_samples=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Print the sampled indices\n",
    "print(sampled_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc923f12-2f0b-4c9a-937b-074f86fff69d",
   "metadata": {},
   "source": [
    "Softmax to Probabilities: F.softmax(pred[0], dim=-1) converts the logits into probabilities. The dim=-1 argument specifies that softmax should be applied across the last dimension (the vocabulary dimension).\n",
    "Sampling: torch.multinomial(probs, num_samples=1) samples one index per timestep from the probability distribution defined by probs. The squeeze() function removes any singleton dimensions, and cpu().numpy() converts the tensor to a NumPy array for easier handling and printing.\n",
    "Output: The resulting sampled_indices are the indices of the predicted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a94bf-3514-42cd-9389-6919d429573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117663ed-45e6-47bb-a36a-5bb690531104",
   "metadata": {},
   "source": [
    "To implement the loss function and training process in PyTorch, we'll use torch.nn.CrossEntropyLoss for the loss calculation, which is equivalent to TensorFlow's sparse_categorical_crossentropy when from_logits=True. Here’s how you can set up the loss function and the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90a6be-66e1-44c0-bc1e-5c911a31f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(labels, logits):\n",
    "    # Flatten the logits and labels for calculating the loss\n",
    "    logits = logits.view(-1, logits.size(-1))\n",
    "    labels = labels.view(-1)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "# Hyperparameters\n",
    "num_training_iterations = 3000  # Number of training iterations\n",
    "batch_size = 8  # Batch size for training\n",
    "seq_length = 100  # Length of each sequence\n",
    "learning_rate = 5e-3  # Learning rate for optimizer\n",
    "embedding_dim = 256  # Embedding dimension\n",
    "rnn_units = 1024  # Number of units in the LSTM layer\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = MusicRNN(vocab_size, embedding_dim, rnn_units, batch_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Checkpoint location:\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
    "\n",
    "# Create the checkpoint directory if it does not exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(num_training_iterations):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    # Get a batch of input and target sequences\n",
    "    x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Reset the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    pred, _ = model(x_batch)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = compute_loss(y_batch, pred)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (iteration + 1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration + 1}/{num_training_iterations}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), checkpoint_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbd950-f6d8-446d-887a-f155a5cc4f21",
   "metadata": {},
   "source": [
    "The training process has completed 3000 iterations, and the model's loss has decreased, indicating that it has learned to predict the next character in the sequences more accurately. Here's a summary of the loss at various points during training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad541262-b336-4f5e-b361-a8ee52249999",
   "metadata": {},
   "source": [
    "Iteration 100: Loss: 1.39\n",
    "Iteration 200: Loss: 1.36\n",
    "Iteration 300: Loss: 1.16\n",
    "Iteration 400: Loss: 1.07\n",
    "Iteration 500: Loss: 1.01\n",
    "Iteration 1000: Loss: 0.91\n",
    "Iteration 1500: Loss: 0.77\n",
    "Iteration 2000: Loss: 0.77\n",
    "Iteration 2500: Loss: 0.76\n",
    "Iteration 3000: Loss: 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2337d6-ff47-44a3-965e-3f8f556ade59",
   "metadata": {},
   "source": [
    "The final loss value of around 0.82 suggests that the model has improved its predictive accuracy. You can now save the model's parameters if you haven't already, and use the trained model for generating new sequences or further evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b788e-e110-4f91-a3cc-71d0bea9f29a",
   "metadata": {},
   "source": [
    "To integrate Comet.ml with your training workflow for experiment tracking, you can use the provided function create_experiment(). This function initializes a new Comet experiment, which allows you to track the progress of your model training, including hyperparameters, metrics, and more. Below is the code snippet you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d368827-298d-44c8-af9f-1b44d5a3641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "params = {\n",
    "    \"num_training_iterations\": 3000,  # Number of iterations for training\n",
    "    \"batch_size\": 32,                 # Batch size for training\n",
    "    \"seq_length\": 100,                # Sequence length for each input\n",
    "    \"learning_rate\": 0.005,           # Learning rate for the optimizer\n",
    "    \"embedding_dim\": 256,             # Dimension of the embedding layer\n",
    "    \"rnn_units\": 1024                 # Number of units in the LSTM layer\n",
    "}\n",
    "\n",
    "def create_experiment():\n",
    "    # End any prior experiments\n",
    "    if 'experiment' in locals():\n",
    "        experiment.end()\n",
    "\n",
    "    # Initiate the comet experiment for tracking\n",
    "    experiment = comet_ml.Experiment(\n",
    "        api_key=COMET_API_KEY,\n",
    "        project_name=\"6S191_Lab1_Part2\"\n",
    "    )\n",
    "    \n",
    "    # Log our hyperparameters to the experiment\n",
    "    for param, value in params.items():\n",
    "        experiment.log_parameter(param, value)\n",
    "    \n",
    "    experiment.flush()\n",
    "    return experiment\n",
    "\n",
    "# Example usage:\n",
    "experiment = create_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec57ff-285f-452e-9767-5a2a0dda1a19",
   "metadata": {},
   "source": [
    "Model and Optimizer Initialization: We'll instantiate a new model and an optimizer.\n",
    "Training Loop: We'll define a training step function and then iterate over a set number of training iterations, logging the loss and saving the model periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df4d69-5329-41cd-b2c6-aa3622d91f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the MusicRNN model class as previously implemented\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.dense(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return (torch.zeros(1, self.batch_size, self.rnn_units).to(device),\n",
    "                torch.zeros(1, self.batch_size, self.rnn_units).to(device))\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(labels, logits):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "# Define optimizer and training parameters\n",
    "params = {\n",
    "    \"num_training_iterations\": 3000,  # Number of iterations for training\n",
    "    \"batch_size\": 32,                 # Batch size for training\n",
    "    \"seq_length\": 100,                # Sequence length for each input\n",
    "    \"learning_rate\": 0.005,           # Learning rate for the optimizer\n",
    "    \"embedding_dim\": 256,             # Dimension of the embedding layer\n",
    "    \"rnn_units\": 1024,                # Number of units in the LSTM layer\n",
    "    \"vocab_size\": len(vocab)          # Vocabulary size\n",
    "}\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = MusicRNN(params[\"vocab_size\"], params[\"embedding_dim\"], params[\"rnn_units\"], params[\"batch_size\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "# Define a training step function\n",
    "def train_step(x, y):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(x)\n",
    "    loss = compute_loss(y, output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "history = []\n",
    "checkpoint_prefix = 'model_checkpoint.pth'\n",
    "for iter in tqdm(range(params[\"num_training_iterations\"])):\n",
    "    x_batch, y_batch = get_batch(vectorized_songs, params[\"seq_length\"], params[\"batch_size\"])\n",
    "    x_batch, y_batch = torch.tensor(x_batch).to(device), torch.tensor(y_batch).to(device)\n",
    "\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "    history.append(loss)\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        torch.save(model.state_dict(), checkpoint_prefix)\n",
    "        print(f'Iteration {iter}/{params[\"num_training_iterations\"]}, Loss: {loss}')\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), checkpoint_prefix)\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd68a23-68c3-4118-b51a-d32508bc874b",
   "metadata": {},
   "source": [
    "To generate music using the trained RNN model in PyTorch, you'll need to follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a066d9-8a42-48a9-887e-d00843915b19",
   "metadata": {},
   "source": [
    "Rebuild the model: Initialize the model with a batch size of 1 for generation.\n",
    "Load the trained weights: Load the weights from the latest checkpoint.\n",
    "Generate music: Use a seed sequence to start generating new characters iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2ea77-ffbc-4495-a817-b97808263445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b8ad1-c5f4-4448-83ba-f96520ed03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the MusicRNN model class (assuming it's already defined above)\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.dense(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return (torch.zeros(1, self.batch_size, self.rnn_units).to(device),\n",
    "                torch.zeros(1, self.batch_size, self.rnn_units).to(device))\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "vocab_size = len(vocab)  # Size of the vocabulary\n",
    "batch_size = 1  # Batch size of 1 for generation\n",
    "\n",
    "# Rebuild the model with batch_size=1\n",
    "model = MusicRNN(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "model.load_state_dict(torch.load('model_checkpoint.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text using the model\n",
    "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
    "    # Convert the start string to numbers (vectorize)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "        # Remove the batch dimension\n",
    "        predictions = predictions[:, -1, :]\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        # Sample the next character using a categorical distribution\n",
    "        predicted_id = torch.multinomial(torch.nn.functional.softmax(predictions, dim=-1), num_samples=1)\n",
    "        predicted_id = predicted_id.item()\n",
    "\n",
    "        # Pass the prediction along with the previous hidden state\n",
    "        input_eval = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        # Add the predicted character to the text\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Generate music\n",
    "start_string = \"X:1\\nT:Generated\\nZ:Example\\nM:C|\\nL:1/8\\nK:C\\n\"\n",
    "generated_text = generate_text(model, start_string)\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a523621-ee90-413a-9477-4388062c8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the function to generate text using the trained model\n",
    "def generate_text(model, start_string, generation_length=1000):\n",
    "    # Evaluation step (generating ABC text using the learned RNN model)\n",
    "\n",
    "    # Convert the start string to numbers (vectorize)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    for i in tqdm(range(generation_length)):\n",
    "        # Evaluate the inputs and generate the next character predictions\n",
    "        predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "        # Remove the batch dimension\n",
    "        predictions = predictions[:, -1, :]\n",
    "        \n",
    "        # Use a multinomial distribution to sample\n",
    "        predicted_id = torch.multinomial(F.softmax(predictions, dim=-1), num_samples=1).item()\n",
    "\n",
    "        # Pass the prediction along with the previous hidden state as the next inputs to the model\n",
    "        input_eval = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        # Add the predicted character to the generated text\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Use the model and the function defined above to generate ABC format text of length 1000!\n",
    "generated_text = generate_text(model, start_string=\"X\", generation_length=1000)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa86006-f5fb-4206-984c-fec4d8e609cc",
   "metadata": {},
   "source": [
    "To convert the generated ABC format text into audio and play back the generated music, you can use the music21 library to parse the ABC notation and then use IPython.display.Audio for playback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923bdc6-5612-40cf-9a54-7be5ad63b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, midi\n",
    "from IPython.display import Audio, display\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def play_song_abc(abc_str, output_file=\"output.wav\"):\n",
    "    try:\n",
    "        # Convert ABC to MIDI\n",
    "        score = converter.parse(abc_str, format='abc')\n",
    "        # Remove duplicate time signatures if present\n",
    "        for element in score.flat.getElementsByClass('TimeSignature'):\n",
    "            score.remove(element, recurse=True)\n",
    "        mf = midi.translate.music21ObjectToMidiFile(score)\n",
    "        midi_filename = 'output.midi'\n",
    "        mf.open(midi_filename, 'wb')\n",
    "        mf.write()\n",
    "        mf.close()\n",
    "\n",
    "        # Convert MIDI to WAV using FluidSynth\n",
    "        sf2_path = '/home/cesco345/musicgen/FluidR3_GM/FluidR3_GM.sf2'  # Replace with your SoundFont file path\n",
    "        if not os.path.exists(sf2_path):\n",
    "            print(\"SoundFont file not found. Please make sure it is installed.\")\n",
    "            return\n",
    "        fs = FluidSynth(sf2_path)\n",
    "        fs.midi_to_audio(midi_filename, output_file)\n",
    "\n",
    "        # Load and play WAV file using IPython.display.Audio\n",
    "        display(Audio(output_file))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the song: {e}\")\n",
    "\n",
    "# Extract the song snippets from the generated text\n",
    "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
    "\n",
    "for i, song in enumerate(generated_songs):\n",
    "    print(\"Generated song\", i)\n",
    "    play_song_abc(song, f\"output_{i}.wav\")\n",
    "    experiment.log_asset(f\"output_{i}.wav\")  # Save the song to the Comet interface\n",
    "\n",
    "# End the Comet experiment\n",
    "experiment.end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f5c14-b9d0-4a22-a107-7abe0b576eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
