{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf20019-5532-45c8-a78b-9b32983e1d48",
   "metadata": {},
   "source": [
    "For this example, we will use the \"Protein Solubility Dataset\" from the UCI Machine Learning Repository, which provides protein sequences labeled as soluble or insoluble. We will encode these sequences into numerical features and use them to train our model.  This lab demonstrates how to use a neural network in PyTorch to predict protein solubility from amino acid composition data. We will use real protein sequences, encode them into numerical features, and train a simple neural network to classify the sequences as soluble or insoluble. This basic framework can be expanded to more complex models and datasets for various biotech applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34b6f1-37b1-41fd-8333-7e776f7eca9d",
   "metadata": {},
   "source": [
    "## Lab 1: Basic Biotech Application of Neural Networks using PyTorch\n",
    "\n",
    "In this Jupyter notebook, we explore a fundamental application of neural networks in biotechnology using PyTorch. This lab demonstrates how neural networks can be used to predict the solubility of proteins based on their amino acid composition.\n",
    "\n",
    "### Part 1: Understanding the Dataset\n",
    "\n",
    "The dataset consists of protein sequences represented by their amino acid composition. Each sequence is encoded into numerical features by calculating the frequency of each of the 20 standard amino acids. The labels indicate whether the protein is soluble (1) or insoluble (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a5186-8212-429c-b972-7d40dcb8072d",
   "metadata": {},
   "source": [
    "**Dataset Loading and Encoding:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6aa3072-82ad-45e7-b48e-0580f5209566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RMSD        F1       F2       F3        F4            F5        F6  \\\n",
      "0  17.284  13558.30  4305.35  0.31754  162.1730  1.872791e+06  215.3590   \n",
      "1   6.021   6191.96  1623.16  0.26213   53.3894  8.034467e+05   87.2024   \n",
      "2   9.275   7725.98  1726.28  0.22343   67.2887  1.075648e+06   81.7913   \n",
      "3  15.851   8424.58  2368.25  0.28111   67.8325  1.210472e+06  109.4390   \n",
      "4   7.962   7460.84  1736.94  0.23280   52.4123  1.021020e+06   94.5234   \n",
      "\n",
      "        F7   F8       F9  \n",
      "0  4287.87  102  27.0302  \n",
      "1  3328.91   39  38.5468  \n",
      "2  2981.04   29  38.8119  \n",
      "3  3248.22   70  39.0651  \n",
      "4  2814.42   41  39.9147  \n",
      "tensor([[1.7284e+01, 1.3558e+04, 4.3054e+03, 3.1754e-01, 1.6217e+02, 1.8728e+06,\n",
      "         2.1536e+02, 4.2879e+03, 1.0200e+02],\n",
      "        [6.0210e+00, 6.1920e+03, 1.6232e+03, 2.6213e-01, 5.3389e+01, 8.0345e+05,\n",
      "         8.7202e+01, 3.3289e+03, 3.9000e+01],\n",
      "        [9.2750e+00, 7.7260e+03, 1.7263e+03, 2.2343e-01, 6.7289e+01, 1.0756e+06,\n",
      "         8.1791e+01, 2.9810e+03, 2.9000e+01],\n",
      "        [1.5851e+01, 8.4246e+03, 2.3682e+03, 2.8111e-01, 6.7832e+01, 1.2105e+06,\n",
      "         1.0944e+02, 3.2482e+03, 7.0000e+01],\n",
      "        [7.9620e+00, 7.4608e+03, 1.7369e+03, 2.3280e-01, 5.2412e+01, 1.0210e+06,\n",
      "         9.4523e+01, 2.8144e+03, 4.1000e+01]])\n",
      "tensor([[ 5074.],\n",
      "        [27423.],\n",
      "        [28086.],\n",
      "        [28804.],\n",
      "        [30760.]])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset (replace 'dataset_url' with the actual URL or file path)\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\"\n",
    "data = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the last column contains the labels and all preceding columns are features\n",
    "features = data.iloc[:, :-1].values  # All columns except the last one\n",
    "labels = data.iloc[:, -1].values     # The last column\n",
    "\n",
    "# Encode the labels (if necessary, here we assume they are already numerical)\n",
    "# If labels are categorical strings, you might need to use LabelEncoder, as shown below\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(encoded_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Display the first few feature vectors and labels for verification\n",
    "print(features[:5])\n",
    "print(labels[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639294d2-e20a-4d2c-9112-b18d2c0bf904",
   "metadata": {},
   "source": [
    "The columns RMSD, F1, F2, F3, F4, F5, F6, F7, F8, and F9 are likely features or variables extracted from the dataset related to protein structures. Here's a general interpretation of these terms, which could vary depending on the specific context of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af046bc-8d16-407e-b017-8cce9cb5bb82",
   "metadata": {},
   "source": [
    "    RMSD (Root Mean Square Deviation):\n",
    "        In protein structure analysis, RMSD is a measure of the average distance between atoms (usually the backbone atoms) of superimposed proteins. It is commonly used to assess the structural similarity between two protein conformations.\n",
    "\n",
    "    F1, F2, F3, ... F9:\n",
    "        These are likely features extracted from the dataset. In the context of protein analysis, such features can include various structural, physicochemical, or sequence-based properties of proteins.\n",
    "\n",
    "    Common Features in Protein Data:\n",
    "        In general, datasets involving proteins may include features such as:\n",
    "            Hydrophobicity: A measure of how hydrophobic (water-repellent) a molecule or part of a molecule is.\n",
    "            Amino Acid Composition: The relative abundance of each amino acid in the protein sequence.\n",
    "            Secondary Structure Content: The proportion of alpha-helices, beta-sheets, etc.\n",
    "            Molecular Weight: The mass of the protein or its fragments.\n",
    "            Surface Accessibility: The degree to which certain residues or regions are exposed to the solvent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b9636-1f70-442e-9695-858f16b78258",
   "metadata": {},
   "source": [
    "    Loading and Inspecting the Data:\n",
    "        The dataset is loaded using pd.read_csv(). The print(data.head()) statement helps us inspect the first few rows to verify the structure.\n",
    "\n",
    "    Extracting Features and Labels:\n",
    "        features = data.iloc[:, :-1].values: This selects all columns except the last one, assuming these are the features.\n",
    "        labels = data.iloc[:, -1].values: This selects the last column as the labels.\n",
    "\n",
    "    Encoding Labels (if necessary):\n",
    "        If the labels are categorical strings, LabelEncoder is used to convert them into numerical format. If they are already numerical, this step may be unnecessary.\n",
    "\n",
    "    Conversion to PyTorch Tensors:\n",
    "        The features and labels are converted to PyTorch tensors using torch.tensor(), which is necessary for training with PyTorch.\n",
    "\n",
    "    Verification:\n",
    "        The first few entries of the features and labels are printed to ensure they have been loaded and processed correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f53601-0f21-4d94-8ee7-3e675a9ee3c4",
   "metadata": {},
   "source": [
    "### Part 2: Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b8232e-3206-4033-9835-5a01d3195677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        # Define a hidden layer with 50 units\n",
    "        self.hidden = nn.Linear(input_size, 50)\n",
    "        # Define an output layer with 1 unit\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        # Sigmoid activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation function after hidden layer\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        # Apply the output layer and then sigmoid activation function\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Define the input size based on the number of features\n",
    "input_size = features.shape[1]\n",
    "model = SolubilityPredictor(input_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74df42-68c1-4ac0-82d6-459e68420266",
   "metadata": {},
   "source": [
    "Imports:\n",
    "\n",
    "    torch: Main PyTorch library.\n",
    "    torch.nn: Module containing neural network layers and loss functions.\n",
    "\n",
    "SolubilityPredictor Class:\n",
    "\n",
    "    This class inherits from torch.nn.Module, making it a PyTorch neural network model.\n",
    "    __init__(self, input_size):\n",
    "        This method initializes the model, setting up the layers.\n",
    "        self.hidden = nn.Linear(input_size, 50): Creates a fully connected layer with input_size inputs and 50 outputs.\n",
    "        self.output = nn.Linear(50, 1): Creates a fully connected output layer with 50 inputs and 1 output.\n",
    "        self.sigmoid = nn.Sigmoid(): A sigmoid activation function to ensure the output is between 0 and 1.\n",
    "\n",
    "Forward Method:\n",
    "\n",
    "    forward(self, x): This method defines the forward pass of the network. It determines how data flows through the network.\n",
    "    x = torch.relu(self.hidden(x)): Applies a ReLU activation function after the hidden layer.\n",
    "    x = self.output(x): Passes the data through the output layer.\n",
    "    return self.sigmoid(x): Applies a sigmoid activation function to the output, useful for binary classification tasks where the output needs to be between 0 and 1.\n",
    "\n",
    "Model Instantiation:\n",
    "\n",
    "    input_size = features.shape[1]: The number of input features is determined from the dataset.\n",
    "    model = SolubilityPredictor(input_size): An instance of the SolubilityPredictor model is created with the specified input size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4479b1-8a72-48de-bc68-70c7a13c118d",
   "metadata": {},
   "source": [
    "This corrected approach uses the dataset's numerical features directly for training a neural network, appropriate for tasks such as regression or classification depending on the label's nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626f18c-bee5-4d2f-8bc4-7b9fafa59fa4",
   "metadata": {},
   "source": [
    "This neural network model is designed for a binary classification task, predicting protein solubility. The ReLU activation function is used in the hidden layer to introduce non-linearity, and the sigmoid activation function in the output layer ensures that the output is a probability value between 0 and 1, suitable for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cbc43-7f37-4f3f-98cf-24a2e779eb73",
   "metadata": {},
   "source": [
    "### Part 3: Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec09cb-1d7f-4f35-868f-6429134e0686",
   "metadata": {},
   "source": [
    "We train the model using the dataset. The loss function used is binary cross-entropy, and we use stochastic gradient descent (SGD) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed8c338b-c8b0-4052-9b68-56891cbf0518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label range: 0.0 to 1.0\n",
      "Epoch [10/100], Loss: 0.0022\n",
      "Epoch [20/100], Loss: 0.0022\n",
      "Epoch [30/100], Loss: 0.0022\n",
      "Epoch [40/100], Loss: 0.0022\n",
      "Epoch [50/100], Loss: 0.0022\n",
      "Epoch [60/100], Loss: 0.0022\n",
      "Epoch [70/100], Loss: 0.0022\n",
      "Epoch [80/100], Loss: 0.0022\n",
      "Epoch [90/100], Loss: 0.0022\n",
      "Epoch [100/100], Loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming labels are in the range [0, class_count - 1] (e.g., 0, 1, 2, ...)\n",
    "# Normalize labels to [0, 1] range if they are not already\n",
    "if labels.max() > 1:\n",
    "    labels = (labels == labels.max()).float()\n",
    "\n",
    "# Check the label range\n",
    "print(f\"Label range: {labels.min()} to {labels.max()}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3afdac-8e09-458e-acaf-f3174b1beb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 4: Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b27ab4b4-f5a6-45a6-a55e-99810ec5c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    predictions = model(features)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    accuracy = (predicted_classes == labels).sum() / len(labels)\n",
    "    print(f'Accuracy: {accuracy.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5be74e65-de8a-4026-8550-d3fa102103ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RMSD        F1       F2       F3        F4            F5        F6  \\\n",
      "0  17.284  13558.30  4305.35  0.31754  162.1730  1.872791e+06  215.3590   \n",
      "1   6.021   6191.96  1623.16  0.26213   53.3894  8.034467e+05   87.2024   \n",
      "2   9.275   7725.98  1726.28  0.22343   67.2887  1.075648e+06   81.7913   \n",
      "3  15.851   8424.58  2368.25  0.28111   67.8325  1.210472e+06  109.4390   \n",
      "4   7.962   7460.84  1736.94  0.23280   52.4123  1.021020e+06   94.5234   \n",
      "\n",
      "        F7   F8       F9  \n",
      "0  4287.87  102  27.0302  \n",
      "1  3328.91   39  38.5468  \n",
      "2  2981.04   29  38.8119  \n",
      "3  3248.22   70  39.0651  \n",
      "4  2814.42   41  39.9147  \n",
      "tensor([[1.7284e+01, 1.3558e+04, 4.3054e+03, 3.1754e-01, 1.6217e+02, 1.8728e+06,\n",
      "         2.1536e+02, 4.2879e+03, 1.0200e+02],\n",
      "        [6.0210e+00, 6.1920e+03, 1.6232e+03, 2.6213e-01, 5.3389e+01, 8.0345e+05,\n",
      "         8.7202e+01, 3.3289e+03, 3.9000e+01],\n",
      "        [9.2750e+00, 7.7260e+03, 1.7263e+03, 2.2343e-01, 6.7289e+01, 1.0756e+06,\n",
      "         8.1791e+01, 2.9810e+03, 2.9000e+01],\n",
      "        [1.5851e+01, 8.4246e+03, 2.3682e+03, 2.8111e-01, 6.7832e+01, 1.2105e+06,\n",
      "         1.0944e+02, 3.2482e+03, 7.0000e+01],\n",
      "        [7.9620e+00, 7.4608e+03, 1.7369e+03, 2.3280e-01, 5.2412e+01, 1.0210e+06,\n",
      "         9.4523e+01, 2.8144e+03, 4.1000e+01]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Epoch [10/100], Loss: 0.0022\n",
      "Epoch [20/100], Loss: 0.0022\n",
      "Epoch [30/100], Loss: 0.0022\n",
      "Epoch [40/100], Loss: 0.0022\n",
      "Epoch [50/100], Loss: 0.0022\n",
      "Epoch [60/100], Loss: 0.0022\n",
      "Epoch [70/100], Loss: 0.0022\n",
      "Epoch [80/100], Loss: 0.0022\n",
      "Epoch [90/100], Loss: 0.0022\n",
      "Epoch [100/100], Loss: 0.0022\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset (replace 'dataset_url' with the actual URL or file path)\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\"\n",
    "data = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the last column contains the labels and all preceding columns are features\n",
    "features = data.iloc[:, :-1].values  # All columns except the last one\n",
    "labels = data.iloc[:, -1].values     # The last column\n",
    "\n",
    "# Encode the labels if they are not binary (if necessary)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Check if labels are binary, convert to float if necessary\n",
    "if encoded_labels.max() > 1:\n",
    "    encoded_labels = (encoded_labels == encoded_labels.max()).astype(float)\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(encoded_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Display the first few feature vectors and labels for verification\n",
    "print(features[:5])\n",
    "print(labels[:5])\n",
    "\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 100)\n",
    "        self.hidden2 = nn.Linear(100, 50)\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Define the input size based on the number of features\n",
    "input_size = features.shape[1]\n",
    "model = SolubilityPredictor(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    predictions = model(features)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    accuracy = (predicted_classes == labels).sum() / len(labels)\n",
    "    print(f'Accuracy: {accuracy.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956ca96-8bb8-471d-8570-303fef96a1a7",
   "metadata": {},
   "source": [
    "The provided outputs indicate that the model is training and achieving an accuracy of 100%, with a very low and stable loss throughout the epochs. This result suggests that the model may be overfitting the training data, especially since the dataset's labels are all zeros, leading to a lack of diversity in the target values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde949-bdbe-4a82-aca0-74fd3f646aa4",
   "metadata": {},
   "source": [
    "Key Observations and Considerations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b5fe7-5b65-448d-b44d-3f8742ff53b0",
   "metadata": {},
   "source": [
    "    All Zero Labels:\n",
    "        The labels tensor indicates that all samples have a label of 0. This means the model might only learn to predict the majority class, which is 0, leading to artificially high accuracy.\n",
    "\n",
    "    Overfitting:\n",
    "        With a consistent loss and perfect accuracy, the model may not be generalizing well to unseen data. Overfitting occurs when the model learns to predict training data perfectly but fails to generalize to new, unseen data.\n",
    "\n",
    "    Data Imbalance or Labeling Issue:\n",
    "        If all labels are zero, there might be a problem with data imbalance or an issue in how labels were processed or loaded. In a real-world scenario, it's important to ensure that labels correctly represent the classes you're trying to predict.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "    Verify Label Distribution:\n",
    "        Check the label distribution to ensure diversity. If there's an imbalance, consider using techniques like oversampling, undersampling, or adjusting class weights during training.\n",
    "\n",
    "    Use a Validation Set:\n",
    "        Implement a validation set to monitor the model's performance on unseen data. This helps identify overfitting and ensures the model generalizes well.\n",
    "\n",
    "    Introduce Regularization:\n",
    "        If overfitting is suspected, introduce regularization techniques such as dropout, weight decay, or early stopping.\n",
    "\n",
    "    Check Data Processing:\n",
    "        Ensure that the data processing pipeline, especially the labeling process, is correct and that the labels accurately reflect the desired output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863dfe3-8dcb-4b50-97a5-35b6208ebfdd",
   "metadata": {},
   "source": [
    "Here’s a brief example to check label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d651089-7d52-4a89-9745-aa9a87132779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution: {0.0: 45729, 1.0: 1}\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "print(f\"Label Distribution: {dict(zip(unique_labels.tolist(), counts.tolist()))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7121a65-c229-48b0-8327-6257f51caa66",
   "metadata": {},
   "source": [
    "And here's an example of how to split the data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "978663ea-564d-49b4-982a-e913b82b9def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RMSD        F1       F2       F3        F4            F5        F6  \\\n",
      "0  17.284  13558.30  4305.35  0.31754  162.1730  1.872791e+06  215.3590   \n",
      "1   6.021   6191.96  1623.16  0.26213   53.3894  8.034467e+05   87.2024   \n",
      "2   9.275   7725.98  1726.28  0.22343   67.2887  1.075648e+06   81.7913   \n",
      "3  15.851   8424.58  2368.25  0.28111   67.8325  1.210472e+06  109.4390   \n",
      "4   7.962   7460.84  1736.94  0.23280   52.4123  1.021020e+06   94.5234   \n",
      "\n",
      "        F7   F8       F9  \n",
      "0  4287.87  102  27.0302  \n",
      "1  3328.91   39  38.5468  \n",
      "2  2981.04   29  38.8119  \n",
      "3  3248.22   70  39.0651  \n",
      "4  2814.42   41  39.9147  \n",
      "tensor([[1.7284e+01, 1.3558e+04, 4.3054e+03, 3.1754e-01, 1.6217e+02, 1.8728e+06,\n",
      "         2.1536e+02, 4.2879e+03, 1.0200e+02],\n",
      "        [6.0210e+00, 6.1920e+03, 1.6232e+03, 2.6213e-01, 5.3389e+01, 8.0345e+05,\n",
      "         8.7202e+01, 3.3289e+03, 3.9000e+01],\n",
      "        [9.2750e+00, 7.7260e+03, 1.7263e+03, 2.2343e-01, 6.7289e+01, 1.0756e+06,\n",
      "         8.1791e+01, 2.9810e+03, 2.9000e+01],\n",
      "        [1.5851e+01, 8.4246e+03, 2.3682e+03, 2.8111e-01, 6.7832e+01, 1.2105e+06,\n",
      "         1.0944e+02, 3.2482e+03, 7.0000e+01],\n",
      "        [7.9620e+00, 7.4608e+03, 1.7369e+03, 2.3280e-01, 5.2412e+01, 1.0210e+06,\n",
      "         9.4523e+01, 2.8144e+03, 4.1000e+01]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Epoch [10/100], Loss: 0.0027\n",
      "Epoch [20/100], Loss: 0.0027\n",
      "Epoch [30/100], Loss: 0.0027\n",
      "Epoch [40/100], Loss: 0.0027\n",
      "Epoch [50/100], Loss: 0.0027\n",
      "Epoch [60/100], Loss: 0.0027\n",
      "Epoch [70/100], Loss: 0.0027\n",
      "Epoch [80/100], Loss: 0.0027\n",
      "Epoch [90/100], Loss: 0.0027\n",
      "Epoch [100/100], Loss: 0.0027\n",
      "Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (replace 'dataset_url' with the actual URL or file path)\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\"\n",
    "data = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the last column contains the labels and all preceding columns are features\n",
    "features = data.iloc[:, :-1].values  # All columns except the last one\n",
    "labels = data.iloc[:, -1].values     # The last column\n",
    "\n",
    "# Encode the labels if they are not binary (if necessary)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Check if labels are binary, convert to float if necessary\n",
    "if encoded_labels.max() > 1:\n",
    "    encoded_labels = (encoded_labels == encoded_labels.max()).astype(float)\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(encoded_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Display the first few feature vectors and labels for verification\n",
    "print(features[:5])\n",
    "print(labels[:5])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors properly\n",
    "features_train = features_train.clone().detach()\n",
    "labels_train = labels_train.clone().detach().view(-1, 1)\n",
    "features_val = features_val.clone().detach()\n",
    "labels_val = labels_val.clone().detach().view(-1, 1)\n",
    "\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 100)\n",
    "        self.hidden2 = nn.Linear(100, 50)\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Define the input size based on the number of features\n",
    "input_size = features_train.shape[1]\n",
    "model = SolubilityPredictor(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features_train)\n",
    "    loss = criterion(outputs, labels_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    predictions = model(features_val)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    accuracy = (predicted_classes == labels_val).sum() / len(labels_val)\n",
    "    print(f'Validation Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2617e-83f2-4013-a5fc-4885aa119494",
   "metadata": {},
   "source": [
    "The results show that the model is achieving a perfect accuracy of 100% on the validation set, and the loss remains constant at a very low value throughout the training process. Additionally, the labels tensor shows all zeros, indicating that the dataset might be highly imbalanced, with most or all of the labels being the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d1518-f9bb-43d7-baef-89a1982a879b",
   "metadata": {},
   "source": [
    "    Imbalanced Dataset:\n",
    "        The dataset appears to be imbalanced, with only one class present (all zeros). This means that the model learns to predict only the majority class, which results in a misleadingly high accuracy.\n",
    "\n",
    "    Label Distribution Check:\n",
    "        Ensure that the dataset actually contains more than one class. If all labels are zero, the dataset is not suitable for training a binary classification model, as it lacks diversity in the labels.\n",
    "\n",
    "    Feature and Label Verification:\n",
    "        Double-check the source and processing of the labels to ensure that they are being loaded and processed correctly. There may be an issue with how the labels were encoded or extracted from the dataset.\n",
    "\n",
    "    Potential Data Quality Issue:\n",
    "        The issue could stem from a problem in the data acquisition or preprocessing pipeline. Verify the data source and ensure the data accurately reflects the intended features and labels.\n",
    "\n",
    "Corrective Steps:\n",
    "\n",
    "    Inspect the Original Dataset:\n",
    "        Before any processing, ensure the original dataset has the correct labels. It may be necessary to manually inspect a subset of the data to confirm its accuracy.\n",
    "\n",
    "    Re-verify Data Processing Steps:\n",
    "        Go through each step of the data processing pipeline to confirm that features and labels are extracted correctly and that no unintended transformations are applied.\n",
    "\n",
    "    Consider Collecting More Data:\n",
    "        If the dataset truly lacks diversity, consider collecting more data that includes a broader range of classes. This will help in building a model that can generalize well across different classes.\n",
    "\n",
    "    Use a Different Metric:\n",
    "        In the case of imbalanced datasets, accuracy may not be a reliable metric. Consider using other metrics like precision, recall, F1-score, or AUC-ROC to better understand the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be5edd-9573-43fb-9af8-3a9309a1b9ef",
   "metadata": {},
   "source": [
    "### Summary of the Lab\n",
    "\n",
    "**Objective**:\n",
    "The lab aimed to build a neural network using PyTorch to predict an outcome based on given features from a dataset related to protein structures. Specifically, the dataset included features like RMSD and various other measurements (F1, F2, F3, etc.), possibly indicating structural or physicochemical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fdc7d6-c49a-452a-9e1a-f6307d314d52",
   "metadata": {},
   "source": [
    "\n",
    "**Outcome**:\n",
    "After preprocessing the data and training the model, the lab demonstrated the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c9161-0fa9-4c2c-83bc-f08364853cb7",
   "metadata": {},
   "source": [
    "\n",
    "    Data Preparation:\n",
    "        Features and labels were extracted and encoded as necessary. Standardization was applied to the features to ensure numerical stability and efficient training.\n",
    "        The data was split into training and validation sets to evaluate model performance.\n",
    "\n",
    "    Model Training:\n",
    "        A neural network model was defined with two hidden layers, using the ReLU activation function and a sigmoid activation for the output layer.\n",
    "        The model was trained using the Binary Cross Entropy Loss function (BCELoss), which is suitable for binary classification tasks.\n",
    "\n",
    "    Model Evaluation:\n",
    "        The model achieved perfect accuracy on the training and validation datasets. However, this likely indicates an issue with the data, such as imbalance or lack of diversity in the labels, since all labels were zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1cf9a-143e-49fc-a3c0-9b7963e2bd88",
   "metadata": {},
   "source": [
    "### Predictions and Testing the Model\n",
    "\n",
    "Given the data and training results, the model has learned to predict a single class (all zeros in the labels). Therefore, the current model can only predict this single class. However, the approach and framework set up can be adapted to a properly labeled and balanced dataset for predicting protein properties.\n",
    "\n",
    "To test the model on new data, assuming that we have standardized new features similarly to the training data, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8ddd9-7b47-41ef-94ce-e58fad1a87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example new data (replace this with actual new data)\n",
    "new_data = [\n",
    "    [10.5, 9000.0, 2500.0, 0.3, 100.0, 1.5e6, 150.0, 3000.0, 60, 35.0],\n",
    "    [5.0, 3000.0, 1000.0, 0.2, 50.0, 0.8e6, 80.0, 2000.0, 40, 25.0]\n",
    "]\n",
    "\n",
    "# Standardize new data using the same scaler as the training data\n",
    "new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "new_data_tensor = torch.tensor(new_data_standardized, dtype=torch.float32)\n",
    "\n",
    "# Predict using the trained model\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_data_tensor)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "\n",
    "# Print the predictions\n",
    "print(f\"Predictions: {predictions.numpy()}\")\n",
    "print(f\"Predicted Classes: {predicted_classes.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801db9a-a317-4fb7-a1e7-019479bc6548",
   "metadata": {},
   "source": [
    "And here is the complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22ff522d-afe4-4e44-bd4f-162e3994f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./anaconda3/envs/musicgen/lib/python3.9/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./anaconda3/envs/musicgen/lib/python3.9/site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./anaconda3/envs/musicgen/lib/python3.9/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./anaconda3/envs/musicgen/lib/python3.9/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/envs/musicgen/lib/python3.9/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "143e529f-3374-439f-98b3-9654fc253c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RMSD        F1       F2       F3        F4            F5        F6  \\\n",
      "0  17.284  13558.30  4305.35  0.31754  162.1730  1.872791e+06  215.3590   \n",
      "1   6.021   6191.96  1623.16  0.26213   53.3894  8.034467e+05   87.2024   \n",
      "2   9.275   7725.98  1726.28  0.22343   67.2887  1.075648e+06   81.7913   \n",
      "3  15.851   8424.58  2368.25  0.28111   67.8325  1.210472e+06  109.4390   \n",
      "4   7.962   7460.84  1736.94  0.23280   52.4123  1.021020e+06   94.5234   \n",
      "\n",
      "        F7   F8       F9  \n",
      "0  4287.87  102  27.0302  \n",
      "1  3328.91   39  38.5468  \n",
      "2  2981.04   29  38.8119  \n",
      "3  3248.22   70  39.0651  \n",
      "4  2814.42   41  39.9147  \n",
      "Label Distribution: {0.0: 45729, 1.0: 1}\n",
      "tensor([[ 1.5585e+00,  9.0848e-01,  8.7958e-01,  2.4089e-01,  1.0588e+00,\n",
      "          8.9444e-01,  9.9604e-01,  1.4954e-01,  5.6688e-01],\n",
      "        [-2.8236e-01, -9.0674e-01, -9.5213e-01, -6.4024e-01, -9.0399e-01,\n",
      "         -1.0015e+00, -8.3481e-01, -3.3149e-01, -5.4830e-01],\n",
      "        [ 2.4950e-01, -5.2873e-01, -8.8170e-01, -1.2557e+00, -6.5321e-01,\n",
      "         -5.1886e-01, -9.1212e-01, -5.0599e-01, -7.2531e-01],\n",
      "        [ 1.3243e+00, -3.5658e-01, -4.4329e-01, -3.3842e-01, -6.4340e-01,\n",
      "         -2.7982e-01, -5.1714e-01, -3.7197e-01,  4.4128e-04],\n",
      "        [ 3.4891e-02, -5.9406e-01, -8.7442e-01, -1.1067e+00, -9.2162e-01,\n",
      "         -6.1571e-01, -7.3023e-01, -5.8957e-01, -5.1290e-01]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Epoch [10/100], Loss: 0.1671\n",
      "Epoch [20/100], Loss: 0.1456\n",
      "Epoch [30/100], Loss: 0.1245\n",
      "Epoch [40/100], Loss: 0.1076\n",
      "Epoch [50/100], Loss: 0.0967\n",
      "Epoch [60/100], Loss: 0.0903\n",
      "Epoch [70/100], Loss: 0.0866\n",
      "Epoch [80/100], Loss: 0.0843\n",
      "Epoch [90/100], Loss: 0.0829\n",
      "Epoch [100/100], Loss: 0.0820\n",
      "Validation Accuracy: 0.9957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      1.00      9224\n",
      "     Class 1       0.99      1.00      1.00      9068\n",
      "\n",
      "    accuracy                           1.00     18292\n",
      "   macro avg       1.00      1.00      1.00     18292\n",
      "weighted avg       1.00      1.00      1.00     18292\n",
      "\n",
      "Predictions: [[0.03835765]\n",
      " [0.00028445]]\n",
      "Predicted Classes: [[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Load the dataset (replace 'dataset_url' with the actual URL or file path)\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\"\n",
    "data = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the last column contains the labels and all preceding columns are features\n",
    "features = data.iloc[:, :-1].values  # All columns except the last one\n",
    "labels = data.iloc[:, -1].values     # The last column\n",
    "\n",
    "# Encode the labels if they are not binary (if necessary)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Check if labels are binary, convert to float if necessary\n",
    "if encoded_labels.max() > 1:\n",
    "    encoded_labels = (encoded_labels == encoded_labels.max()).astype(float)\n",
    "\n",
    "# Check label distribution\n",
    "unique_labels, counts = torch.unique(torch.tensor(encoded_labels), return_counts=True)\n",
    "print(f\"Label Distribution: {dict(zip(unique_labels.tolist(), counts.tolist()))}\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Apply RandomOverSampler to balance the classes\n",
    "ros = RandomOverSampler()\n",
    "features_balanced, labels_balanced = ros.fit_resample(features_standardized, encoded_labels)\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "features_tensor = torch.tensor(features_balanced, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_balanced, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Display the first few feature vectors and labels for verification\n",
    "print(features_tensor[:5])\n",
    "print(labels_tensor[:5])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(features_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 100)\n",
    "        self.hidden2 = nn.Linear(100, 50)\n",
    "        self.output = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Define the input size based on the number of features\n",
    "input_size = features_train.shape[1]\n",
    "model = SolubilityPredictor(input_size)\n",
    "\n",
    "# Define the loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss()(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features_train)\n",
    "    loss = criterion(outputs, labels_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(features_val)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    accuracy = (predicted_classes == labels_val).sum() / len(labels_val)\n",
    "    print(f'Validation Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(labels_val.numpy(), predicted_classes.numpy(), target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Testing with new data\n",
    "new_data = [\n",
    "    [10.5, 9000.0, 2500.0, 0.3, 100.0, 1.5e6, 150.0, 3000.0, 60],\n",
    "    [5.0, 3000.0, 1000.0, 0.2, 50.0, 0.8e6, 80.0, 2000.0, 40]\n",
    "]\n",
    "\n",
    "# Standardize new data using the same scaler as the training data\n",
    "new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "new_data_tensor = torch.tensor(new_data_standardized, dtype=torch.float32)\n",
    "\n",
    "# Predict using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_data_tensor)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "\n",
    "# Print the predictions\n",
    "print(f\"Predictions: {predictions.numpy()}\")\n",
    "print(f\"Predicted Classes: {predicted_classes.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e88b19-0be1-4bd7-a98c-03e45af81820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
